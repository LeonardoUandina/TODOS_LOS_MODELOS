{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d970889c68ca4550a37dfb864c02ca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a46f6eddecd4adb9bc0fcc9b95b184b",
              "IPY_MODEL_d919c2dd9e9d4b0086101c4e62e4d0e9",
              "IPY_MODEL_5051618163054575bd5f3b829d86a2b4"
            ],
            "layout": "IPY_MODEL_8080644d560b4a9ea6c3cacc53e0bdbb"
          }
        },
        "6a46f6eddecd4adb9bc0fcc9b95b184b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c794f2199044f9bfa10a9147ffb382",
            "placeholder": "​",
            "style": "IPY_MODEL_cea488f6e6864053a71e6e7197c5d187",
            "value": "README.md: "
          }
        },
        "d919c2dd9e9d4b0086101c4e62e4d0e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e592a65428a345a6bdb61ccb68e97261",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1e138f0b20049228a8277d63ecee015",
            "value": 1
          }
        },
        "5051618163054575bd5f3b829d86a2b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecf57a5b6a5d4e098c1dce9fe045b097",
            "placeholder": "​",
            "style": "IPY_MODEL_946e84719d5749df99c84a9cca08fab1",
            "value": " 28.1k/? [00:00&lt;00:00, 1.73MB/s]"
          }
        },
        "8080644d560b4a9ea6c3cacc53e0bdbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10c794f2199044f9bfa10a9147ffb382": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cea488f6e6864053a71e6e7197c5d187": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e592a65428a345a6bdb61ccb68e97261": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b1e138f0b20049228a8277d63ecee015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecf57a5b6a5d4e098c1dce9fe045b097": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946e84719d5749df99c84a9cca08fab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef63c0be7cc04af1960eb2d9083caf5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8580744c6ca440a5841656743d88f16d",
              "IPY_MODEL_d6089523be4d42ceb018882f30655837",
              "IPY_MODEL_b31f1402905042688b2ef89aa0c3cedb"
            ],
            "layout": "IPY_MODEL_4760ae93ccc74fc282ab26d6fe557e71"
          }
        },
        "8580744c6ca440a5841656743d88f16d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_78f58dd5f650434994ef1aa05247e778",
            "placeholder": "​",
            "style": "IPY_MODEL_8aab7c7cfe754d3baade85f5ddf2c718",
            "value": "es-fr/train-00000-of-00001.parquet: 100%"
          }
        },
        "d6089523be4d42ceb018882f30655837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca397c430fa04e419bd010c73739e3bd",
            "max": 9164030,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee7831b5fdae4ca69164f3fe12c6f5ea",
            "value": 9164030
          }
        },
        "b31f1402905042688b2ef89aa0c3cedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ca0451a4734b32ad260735d29ad690",
            "placeholder": "​",
            "style": "IPY_MODEL_fbc7cd6f40014fd0834f11b948a8cc18",
            "value": " 9.16M/9.16M [00:01&lt;00:00, 45.8kB/s]"
          }
        },
        "4760ae93ccc74fc282ab26d6fe557e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78f58dd5f650434994ef1aa05247e778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aab7c7cfe754d3baade85f5ddf2c718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca397c430fa04e419bd010c73739e3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee7831b5fdae4ca69164f3fe12c6f5ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0ca0451a4734b32ad260735d29ad690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbc7cd6f40014fd0834f11b948a8cc18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "178f154b03934d10a2a3ac74dda55bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_750512474d6c4e4b89511b475574dd40",
              "IPY_MODEL_b4d1f3d30815473885ddc8d6e7541cdd",
              "IPY_MODEL_1ea983dfd49f44eda2573f8b4e69e84c"
            ],
            "layout": "IPY_MODEL_516cccfce2d748bb87847ebae7da1d7a"
          }
        },
        "750512474d6c4e4b89511b475574dd40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff40b0586f5e45908f6a161fc6a6b57a",
            "placeholder": "​",
            "style": "IPY_MODEL_883ea8079f6144a48e72a85e262a877c",
            "value": "Generating train split: 100%"
          }
        },
        "b4d1f3d30815473885ddc8d6e7541cdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575d96c6c002406faa9f1ceab452e523",
            "max": 56319,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9cd81e833140f0884955a35d2b2697",
            "value": 56319
          }
        },
        "1ea983dfd49f44eda2573f8b4e69e84c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f986d8508bc4b0c9f5b83efe99f60d8",
            "placeholder": "​",
            "style": "IPY_MODEL_809edbc5dc664fe5a2f42a9717eeee69",
            "value": " 56319/56319 [00:00&lt;00:00, 401439.10 examples/s]"
          }
        },
        "516cccfce2d748bb87847ebae7da1d7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff40b0586f5e45908f6a161fc6a6b57a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883ea8079f6144a48e72a85e262a877c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575d96c6c002406faa9f1ceab452e523": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9cd81e833140f0884955a35d2b2697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2f986d8508bc4b0c9f5b83efe99f60d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809edbc5dc664fe5a2f42a9717eeee69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Instalación de librerías**"
      ],
      "metadata": {
        "id": "Ref6KK1UcKag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece sacrebleu datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnrhsRj_gjYz",
        "outputId": "7202722f-4666-484e-ca15-ee00cde82e9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.2.0 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Descargar dataset español–francés (HuggingFace)**\n"
      ],
      "metadata": {
        "id": "UOHGdFcxgt5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Descargando dataset español-francés...\")\n",
        "data = load_dataset(\"opus_books\", \"es-fr\")\n",
        "\n",
        "src = [x[\"translation\"][\"es\"] for x in data[\"train\"]]\n",
        "tgt = [x[\"translation\"][\"fr\"] for x in data[\"train\"]]\n",
        "\n",
        "df = pd.DataFrame({\"src\": src, \"tgt\": tgt})\n",
        "\n",
        "# Normalizar\n",
        "df[\"src\"] = df[\"src\"].str.lower().str.strip()\n",
        "df[\"tgt\"] = df[\"tgt\"].str.lower().str.strip()\n",
        "\n",
        "# Filtrar oraciones muy largas\n",
        "df = df[df[\"src\"].str.len() < 120]\n",
        "df = df[df[\"tgt\"].str.len() < 120]\n",
        "\n",
        "# Tomar máximo 50k\n",
        "df = df.sample(min(50000, len(df)), random_state=42)\n",
        "\n",
        "df.to_csv(\"dataset_es_fr.csv\", index=False)\n",
        "print(\"Dataset listo. Total pares:\", len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252,
          "referenced_widgets": [
            "d970889c68ca4550a37dfb864c02ca90",
            "6a46f6eddecd4adb9bc0fcc9b95b184b",
            "d919c2dd9e9d4b0086101c4e62e4d0e9",
            "5051618163054575bd5f3b829d86a2b4",
            "8080644d560b4a9ea6c3cacc53e0bdbb",
            "10c794f2199044f9bfa10a9147ffb382",
            "cea488f6e6864053a71e6e7197c5d187",
            "e592a65428a345a6bdb61ccb68e97261",
            "b1e138f0b20049228a8277d63ecee015",
            "ecf57a5b6a5d4e098c1dce9fe045b097",
            "946e84719d5749df99c84a9cca08fab1",
            "ef63c0be7cc04af1960eb2d9083caf5d",
            "8580744c6ca440a5841656743d88f16d",
            "d6089523be4d42ceb018882f30655837",
            "b31f1402905042688b2ef89aa0c3cedb",
            "4760ae93ccc74fc282ab26d6fe557e71",
            "78f58dd5f650434994ef1aa05247e778",
            "8aab7c7cfe754d3baade85f5ddf2c718",
            "ca397c430fa04e419bd010c73739e3bd",
            "ee7831b5fdae4ca69164f3fe12c6f5ea",
            "b0ca0451a4734b32ad260735d29ad690",
            "fbc7cd6f40014fd0834f11b948a8cc18",
            "178f154b03934d10a2a3ac74dda55bd6",
            "750512474d6c4e4b89511b475574dd40",
            "b4d1f3d30815473885ddc8d6e7541cdd",
            "1ea983dfd49f44eda2573f8b4e69e84c",
            "516cccfce2d748bb87847ebae7da1d7a",
            "ff40b0586f5e45908f6a161fc6a6b57a",
            "883ea8079f6144a48e72a85e262a877c",
            "575d96c6c002406faa9f1ceab452e523",
            "7f9cd81e833140f0884955a35d2b2697",
            "2f986d8508bc4b0c9f5b83efe99f60d8",
            "809edbc5dc664fe5a2f42a9717eeee69"
          ]
        },
        "id": "vPaARBY0gw3U",
        "outputId": "2f9a8555-dbd7-40e7-9b03-7a826f73b7ab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando dataset español-francés...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d970889c68ca4550a37dfb864c02ca90"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "es-fr/train-00000-of-00001.parquet:   0%|          | 0.00/9.16M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef63c0be7cc04af1960eb2d9083caf5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/56319 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "178f154b03934d10a2a3ac74dda55bd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset listo. Total pares: 32556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Imports + Configuración**"
      ],
      "metadata": {
        "id": "-NMLksKrg_YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import sacrebleu\n",
        "import sentencepiece as spm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando:\", DEVICE)\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZlwYm8ohBS1",
        "outputId": "c04de7bb-9b11-4b07-fe44-7be99caf237d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Cargar dataset y dividir**"
      ],
      "metadata": {
        "id": "jOi74Az2hE8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"dataset_es_fr.csv\")\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=SEED)\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.1, random_state=SEED)\n",
        "\n",
        "print(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpRoaARLhG8m",
        "outputId": "6bfc1841-21d9-4ee9-aaf7-b07c92ef62bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 26370 Val: 2930 Test: 3256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Entrenar SentencePiece**"
      ],
      "metadata": {
        "id": "zEQflEl8hP22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(\"spm_models\", exist_ok=True)\n",
        "\n",
        "SRC_VOCAB = 4000\n",
        "TGT_VOCAB = 4000\n",
        "\n",
        "with open(\"spm_src.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(\"\\n\".join(train_df[\"src\"].tolist()))\n",
        "\n",
        "with open(\"spm_tgt.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(\"\\n\".join(train_df[\"tgt\"].tolist()))\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=spm_src.txt --model_prefix=spm_models/spm_src \"\n",
        "    f\"--vocab_size={SRC_VOCAB} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\"\n",
        ")\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f\"--input=spm_tgt.txt --model_prefix=spm_models/spm_tgt \"\n",
        "    f\"--vocab_size={TGT_VOCAB} --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3\"\n",
        ")\n",
        "\n",
        "print(\"SentencePiece listo.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVaxYkmChRl4",
        "outputId": "0d1d4c83-b8e9-4f4f-be24-ca37736ecd5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentencePiece listo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Cargar tokenizers**"
      ],
      "metadata": {
        "id": "7XGPVfU_di7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp_src = spm.SentencePieceProcessor()\n",
        "sp_tgt = spm.SentencePieceProcessor()\n",
        "\n",
        "sp_src.load(\"spm_models/spm_src.model\")\n",
        "sp_tgt.load(\"spm_models/spm_tgt.model\")\n",
        "\n",
        "PAD = 0\n",
        "BOS = 1\n",
        "EOS = 2\n",
        "\n",
        "def enc_src(t): return [BOS] + sp_src.encode(t, out_type=int) + [EOS]\n",
        "def enc_tgt(t): return [BOS] + sp_tgt.encode(t, out_type=int) + [EOS]\n",
        "\n",
        "def dec_tgt(ids):\n",
        "    if EOS in ids: ids = ids[:ids.index(EOS)]\n",
        "    if ids and ids[0] == BOS: ids = ids[1:]\n",
        "    return sp_tgt.decode(ids)\n"
      ],
      "metadata": {
        "id": "GML-JwBA-wIs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Dataset + DataLoader**"
      ],
      "metadata": {
        "id": "MgzPNoZd-xVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NMTDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.src = df[\"src\"].tolist()\n",
        "        self.tgt = df[\"tgt\"].tolist()\n",
        "\n",
        "    def __len__(self): return len(self.src)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s = enc_src(self.src[idx])\n",
        "        t = enc_tgt(self.tgt[idx])\n",
        "        return torch.tensor(s), torch.tensor(t[:-1]), torch.tensor(t[1:])"
      ],
      "metadata": {
        "id": "3S2dB9tg-0_s"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_batch(seqs):\n",
        "    max_len = max(len(s) for s in seqs)\n",
        "    out = torch.full((len(seqs), max_len), PAD)\n",
        "    for i, s in enumerate(seqs):\n",
        "        out[i, :len(s)] = s\n",
        "    return out\n",
        "\n",
        "def collate(batch):\n",
        "    src, tin, tout = zip(*batch)\n",
        "    src_pad = pad_batch(src)\n",
        "    tin_pad = pad_batch(tin)\n",
        "    tout_pad = pad_batch(tout)\n",
        "\n",
        "    lengths = torch.tensor([len(s) for s in src])\n",
        "    lengths, idx = lengths.sort(descending=True)\n",
        "\n",
        "    return src_pad[idx], lengths, tin_pad[idx], tout_pad[idx]"
      ],
      "metadata": {
        "id": "iyivqKiIeyS3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(NMTDataset(train_df), 64, True, collate_fn=collate)\n",
        "val_loader   = DataLoader(NMTDataset(val_df),   64, False, collate_fn=collate)\n",
        "test_loader  = DataLoader(NMTDataset(test_df),  64, False, collate_fn=collate)"
      ],
      "metadata": {
        "id": "OuBiYaKJe0GX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_batch(seqs):\n",
        "    max_len = max(len(s) for s in seqs)\n",
        "    out = torch.full((len(seqs), max_len), PAD)\n",
        "    for i, s in enumerate(seqs):\n",
        "        out[i, :len(s)] = s\n",
        "    return out\n",
        "\n",
        "def collate(batch):\n",
        "    src, tin, tout = zip(*batch)\n",
        "    src_pad = pad_batch(src)\n",
        "    tin_pad = pad_batch(tin)\n",
        "    tout_pad = pad_batch(tout)\n",
        "\n",
        "    lengths = torch.tensor([len(s) for s in src])\n",
        "    lengths, idx = lengths.sort(descending=True)\n",
        "\n",
        "    return src_pad[idx], lengths, tin_pad[idx], tout_pad[idx]"
      ],
      "metadata": {
        "id": "3g4FUvnLDgjx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **========= MODELO RNN SIMPLE (Encoder–Decoder sin atención) =========**"
      ],
      "metadata": {
        "id": "BJfJviVOdxsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.RNN(emb, hid, batch_first=True)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        x = self.emb(x)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
        "        _, h = self.rnn(packed)\n",
        "        return h"
      ],
      "metadata": {
        "id": "S42yIbdrd-eK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.rnn = nn.RNN(emb, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        x = self.emb(x)\n",
        "        out, h = self.rnn(x, h)\n",
        "        out = self.fc(out)\n",
        "        return out, h"
      ],
      "metadata": {
        "id": "HILRiivFeAJ6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc, dec):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "\n",
        "    def forward(self, src, lengths, tgt_in):\n",
        "        h = self.enc(src, lengths)\n",
        "        out, _ = self.dec(tgt_in, h)\n",
        "        return out\n",
        "\n",
        "    def translate(self, text, max_len=40):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            ids = torch.tensor([enc_src(text)], device=DEVICE)\n",
        "            lengths = torch.tensor([ids.size(1)], device=DEVICE)\n",
        "\n",
        "            h = self.enc(ids, lengths)\n",
        "            cur = torch.tensor([[BOS]], device=DEVICE)\n",
        "\n",
        "            gen = []\n",
        "            for _ in range(max_len):\n",
        "                out, h = self.dec(cur, h)\n",
        "                next_tok = out[0, -1].argmax().item()\n",
        "                if next_tok == EOS:\n",
        "                    break\n",
        "                gen.append(next_tok)\n",
        "                cur = torch.tensor([[next_tok]], device=DEVICE)\n",
        "\n",
        "            return dec_tgt(gen)"
      ],
      "metadata": {
        "id": "BZ4i0WXoeEH7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB = 256\n",
        "HID = 384\n",
        "\n",
        "enc = Encoder(sp_src.get_piece_size(), EMB, HID)\n",
        "dec = Decoder(sp_tgt.get_piece_size(), EMB, HID)\n",
        "model = Seq2Seq(enc, dec).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.0007)"
      ],
      "metadata": {
        "id": "PKbwRSzOeG1j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    tot = 0\n",
        "    for src, lengths, tin, tout in train_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "        tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "\n",
        "        optim.zero_grad()\n",
        "        pred = model(src, lengths, tin)\n",
        "\n",
        "        loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optim.step()\n",
        "        tot += loss.item()\n",
        "    return tot / len(train_loader)\n",
        "\n",
        "def val_epoch():\n",
        "    model.eval()\n",
        "    tot = 0\n",
        "    with torch.no_grad():\n",
        "        for src, lengths, tin, tout in val_loader:\n",
        "            src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "            tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "            pred = model(src, lengths, tin)\n",
        "            loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "            tot += loss.item()\n",
        "    return tot / len(val_loader)"
      ],
      "metadata": {
        "id": "ConGwKJpeJAr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "EPOCHS = 5  # o las que estés usando\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr = train_epoch()\n",
        "    vl = val_epoch()\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(vl)\n",
        "    print(f\"Epoch {ep} | Train: {tr:.4f} | Val: {vl:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JAFOeJCdeLT7",
        "outputId": "3a7930e6-3ab4-4095-fb75-e49377f4a1dd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train 5.0144 | Val 4.4042\n",
            "Epoch 2 | Train 4.1605 | Val 4.0751\n",
            "Epoch 3 | Train 3.8588 | Val 3.9296\n",
            "Epoch 4 | Train 3.6645 | Val 3.8318\n",
            "Epoch 5 | Train 3.5155 | Val 3.7901\n",
            "Epoch 6 | Train 3.3906 | Val 3.7376\n",
            "Epoch 7 | Train 3.2816 | Val 3.7202\n",
            "Epoch 8 | Train 3.1882 | Val 3.7059\n",
            "Epoch 9 | Train 3.1017 | Val 3.6953\n",
            "Epoch 10 | Train 3.0210 | Val 3.7053\n",
            "Epoch 11 | Train 2.9444 | Val 3.7131\n",
            "Epoch 12 | Train 2.8731 | Val 3.7258\n",
            "Epoch 13 | Train 2.8035 | Val 3.7324\n",
            "Epoch 14 | Train 2.7391 | Val 3.7489\n",
            "Epoch 15 | Train 2.6755 | Val 3.7765\n",
            "Epoch 16 | Train 2.6158 | Val 3.7907\n",
            "Epoch 17 | Train 2.5571 | Val 3.8173\n",
            "Epoch 18 | Train 2.5015 | Val 3.8446\n",
            "Epoch 19 | Train 2.4466 | Val 3.8729\n",
            "Epoch 20 | Train 2.3932 | Val 3.8940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Evolución de la pérdida (Train vs Val)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RAIIu2qBlN9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    \"hola, ¿cómo estás?\",\n",
        "    \"mañana voy a estudiar\",\n",
        "    \"me gusta la comida francesa\",\n",
        "    \"estoy aprendiendo modelos de traducción automática\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    print(\"ES:\", t)\n",
        "    print(\"FR:\", model.translate(t))\n",
        "    print(\"-\"*40)"
      ],
      "metadata": {
        "id": "5Vu9jS9te7io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyps, refs = [], []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for src, lengths, tin, tout in test_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "\n",
        "        for i in range(src.size(0)):\n",
        "            ids = src[i][:lengths[i]].tolist()\n",
        "            if EOS in ids:\n",
        "                ids = ids[1:ids.index(EOS)]\n",
        "            else:\n",
        "                ids = ids[1:]\n",
        "            src_text = sp_src.decode(ids)\n",
        "\n",
        "            pred = model.translate(src_text)\n",
        "            gold = sp_tgt.decode([x for x in tout[i].tolist() if x not in [PAD,BOS,EOS]])\n",
        "\n",
        "            hyps.append(pred)\n",
        "            refs.append([gold])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "print(\"BLEU:\", bleu.score)"
      ],
      "metadata": {
        "id": "y64BK3tOe9GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **==== MODELO LSTM (ENCODER - DECODER + ATENCIÓN BAHDANAU) ====**"
      ],
      "metadata": {
        "id": "au1hTjaFfH5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V  = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (1, B, H)\n",
        "        # encoder_outputs: (B, T, H)\n",
        "        hidden = hidden.permute(1, 0, 2)  # (B, 1, H)\n",
        "        score = self.V(torch.tanh(\n",
        "            self.W1(hidden) + self.W2(encoder_outputs)\n",
        "        ))  # (B, T, 1)\n",
        "\n",
        "        attn_weights = torch.softmax(score, dim=1)\n",
        "        context = (attn_weights * encoder_outputs).sum(dim=1)\n",
        "        return context, attn_weights"
      ],
      "metadata": {
        "id": "j8ioHalSfmOr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.lstm = nn.LSTM(emb, hid, batch_first=True)\n",
        "\n",
        "    def forward(self, src, lengths):\n",
        "        x = self.emb(src)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
        "        outputs, (h, c) = self.lstm(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        return outputs, (h, c)"
      ],
      "metadata": {
        "id": "z3IHOYqefpj0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.att = BahdanauAttention(hid)\n",
        "        self.lstm = nn.LSTM(emb + hid, hid, batch_first=True)\n",
        "        self.fc = nn.Linear(hid, vocab)\n",
        "\n",
        "    def forward(self, tgt_in, hidden, encoder_outputs):\n",
        "        h, c = hidden\n",
        "        x = self.emb(tgt_in)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(x.size(1)):\n",
        "            context, _ = self.att(h, encoder_outputs)\n",
        "            lstm_in = torch.cat([x[:, t:t+1, :], context.unsqueeze(1)], dim=2)\n",
        "            out, (h, c) = self.lstm(lstm_in, (h, c))\n",
        "            outputs.append(self.fc(out))\n",
        "\n",
        "        return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "fsIPTDmVfrPd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc, dec):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "\n",
        "    def forward(self, src, lengths, tgt_in):\n",
        "        enc_out, hidden = self.enc(src, lengths)\n",
        "        return self.dec(tgt_in, hidden, enc_out)\n",
        "\n",
        "    def translate(self, text, max_len=40):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            src_ids = torch.tensor([enc_src(text)], device=DEVICE)\n",
        "            lengths = torch.tensor([src_ids.size(1)], device=DEVICE)\n",
        "\n",
        "            enc_out, hidden = self.enc(src_ids, lengths)\n",
        "\n",
        "            cur = torch.tensor([[BOS]], device=DEVICE)\n",
        "            gen = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                out = self.dec(cur, hidden, enc_out)\n",
        "                next_tok = out[0, -1].argmax().item()\n",
        "                if next_tok == EOS:\n",
        "                    break\n",
        "                gen.append(next_tok)\n",
        "                cur = torch.tensor([[next_tok]], device=DEVICE)\n",
        "\n",
        "            return dec_tgt(gen)"
      ],
      "metadata": {
        "id": "Fceqhg-SfuHM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB = 256\n",
        "HID = 512\n",
        "\n",
        "enc = Encoder(sp_src.get_piece_size(), EMB, HID)\n",
        "dec = Decoder(sp_tgt.get_piece_size(), EMB, HID)\n",
        "model = Seq2Seq(enc, dec).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "KvXACGVffvx1"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    tot = 0\n",
        "    for src, lengths, tin, tout in train_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "        tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        pred = model(src, lengths, tin)\n",
        "\n",
        "        loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        tot += loss.item()\n",
        "    return tot / len(train_loader)\n",
        "\n",
        "def val_epoch():\n",
        "    model.eval()\n",
        "    tot = 0\n",
        "    with torch.no_grad():\n",
        "        for src, lengths, tin, tout in val_loader:\n",
        "            src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "            tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "            pred = model(src, lengths, tin)\n",
        "            loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "            tot += loss.item()\n",
        "    return tot / len(val_loader)"
      ],
      "metadata": {
        "id": "2dM89dPsfxh9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "EPOCHS = 15  # o las que estés usando\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr = train_epoch()\n",
        "    vl = val_epoch()\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(vl)\n",
        "    print(f\"Epoch {ep} | Train: {tr:.4f} | Val: {vl:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "Cr7_XT0GfyKs",
        "outputId": "e4f5db66-97b4-4d3e-bd32-0197eb4d3430"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train 5.0879 | Val 4.5571\n",
            "Epoch 2 | Train 4.2391 | Val 4.0505\n",
            "Epoch 3 | Train 3.7880 | Val 3.7408\n",
            "Epoch 4 | Train 3.4574 | Val 3.5559\n",
            "Epoch 5 | Train 3.1914 | Val 3.4038\n",
            "Epoch 6 | Train 2.9581 | Val 3.2908\n",
            "Epoch 7 | Train 2.7396 | Val 3.2198\n",
            "Epoch 8 | Train 2.5450 | Val 3.1727\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3342434093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mvl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {ep} | Train {tr:.4f} | Val {vl:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1737070653.py\u001b[0m in \u001b[0;36mval_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mtot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Evolución de la pérdida (Train vs Val)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nUX6seFklTIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    \"hola, ¿cómo estás?\",\n",
        "    \"mañana voy a estudiar en la biblioteca\",\n",
        "    \"me gusta la comida francesa\",\n",
        "    \"estoy aprendiendo modelos de traducción automática\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    print(\"ES:\", t)\n",
        "    print(\"FR:\", model.translate(t))\n",
        "    print(\"-\"*40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5shT8zIf5Pu",
        "outputId": "2cd54fda-d624-45f8-de2e-a7be9c29aa50"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ES: hola, ¿cómo estás?\n",
            "FR: la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant, la pauvre enfant,\n",
            "----------------------------------------\n",
            "ES: mañana voy a estudiar en la biblioteca\n",
            "FR: -- je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain je vais demain\n",
            "----------------------------------------\n",
            "ES: me gusta la comida francesa\n",
            "FR: je suis la température je suis la température je suis la température je suis la température je suis la température je suis la température je suis la température je suis la température je suis la température je suis la température\n",
            "----------------------------------------\n",
            "ES: estoy aprendiendo modelos de traducción automática\n",
            "FR: je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis je suis\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyps, refs = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, lengths, tin, tout in test_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "        for i in range(src.size(0)):\n",
        "            ids = src[i][:lengths[i]].tolist()\n",
        "            if EOS in ids:\n",
        "                ids = ids[1:ids.index(EOS)]\n",
        "            else:\n",
        "                ids = ids[1:]\n",
        "\n",
        "            src_txt = sp_src.decode(ids)\n",
        "            hyp = model.translate(src_txt)\n",
        "            gold = sp_tgt.decode([x for x in tout[i].tolist()\n",
        "                                  if x not in [PAD, BOS, EOS]])\n",
        "\n",
        "            hyps.append(hyp)\n",
        "            refs.append([gold])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "print(\"BLEU:\", bleu.score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9HjCb5yf6n9",
        "outputId": "b65323fb-b753-4e18-e10a-8c011205d2bc"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU: 0.09789450333061166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **====== MODELO GRU (ENCODER - DECODER + ATENCIÓN LUONG) ======**"
      ],
      "metadata": {
        "id": "6MZgxaYUga5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LuongAttention(nn.Module):\n",
        "    def __init__(self, hidden):\n",
        "        super().__init__()\n",
        "        self.hidden = hidden\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # hidden: (1, B, H)\n",
        "        # encoder_outputs: (B, T, H)\n",
        "        hidden = hidden.permute(1, 0, 2)   # (B, 1, H)\n",
        "\n",
        "        # dot score\n",
        "        scores = torch.bmm(encoder_outputs, hidden.transpose(1, 2))  # (B, T, 1)\n",
        "\n",
        "        attn_weights = torch.softmax(scores, dim=1)  # (B, T, 1)\n",
        "        context = torch.sum(attn_weights * encoder_outputs, dim=1)  # (B, H)\n",
        "\n",
        "        return context, attn_weights"
      ],
      "metadata": {
        "id": "bCCrZb6pgehR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.gru = nn.GRU(emb, hid, batch_first=True)\n",
        "\n",
        "    def forward(self, src, lengths):\n",
        "        x = self.emb(src)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(x, lengths.cpu(), batch_first=True)\n",
        "        outputs, h = self.gru(packed)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "        return outputs, h"
      ],
      "metadata": {
        "id": "OJDdVMOAg4D7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab, emb, hid):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab, emb, padding_idx=PAD)\n",
        "        self.att = LuongAttention(hid)\n",
        "        self.gru = nn.GRU(emb + hid, hid, batch_first=True)\n",
        "        self.fc  = nn.Linear(hid, vocab)\n",
        "\n",
        "    def forward(self, tgt_in, h, encoder_outputs):\n",
        "        x = self.emb(tgt_in)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(x.size(1)):\n",
        "            context, _ = self.att(h, encoder_outputs)\n",
        "            gru_in = torch.cat([x[:, t:t+1, :], context.unsqueeze(1)], dim=2)\n",
        "            out, h = self.gru(gru_in, h)\n",
        "            outputs.append(self.fc(out))\n",
        "        return torch.cat(outputs, dim=1)"
      ],
      "metadata": {
        "id": "dl1m7ILwg5s7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, enc, dec):\n",
        "        super().__init__()\n",
        "        self.enc = enc\n",
        "        self.dec = dec\n",
        "\n",
        "    def forward(self, src, lengths, tgt_in):\n",
        "        enc_out, h = self.enc(src, lengths)\n",
        "        return self.dec(tgt_in, h, enc_out)\n",
        "\n",
        "    def translate(self, text, max_len=40):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            ids = torch.tensor([enc_src(text)], device=DEVICE)\n",
        "            lengths = torch.tensor([ids.size(1)], device=DEVICE)\n",
        "\n",
        "            enc_out, h = self.enc(ids, lengths)\n",
        "            cur = torch.tensor([[BOS]], device=DEVICE)\n",
        "            gen = []\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                out = self.dec(cur, h, enc_out)\n",
        "                nxt = out[0, -1].argmax().item()\n",
        "                if nxt == EOS: break\n",
        "                gen.append(nxt)\n",
        "                cur = torch.tensor([[nxt]], device=DEVICE)\n",
        "\n",
        "            return dec_tgt(gen)"
      ],
      "metadata": {
        "id": "avaMBDJag7is"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMB = 256\n",
        "HID = 512\n",
        "\n",
        "enc = Encoder(sp_src.get_piece_size(), EMB, HID)\n",
        "dec = Decoder(sp_tgt.get_piece_size(), EMB, HID)\n",
        "model = Seq2Seq(enc, dec).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=0.0005)"
      ],
      "metadata": {
        "id": "nItooUO1g9Gs"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for src, lengths, tin, tout in train_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "        tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        pred = model(src, lengths, tin)\n",
        "        loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        total += loss.item()\n",
        "    return total / len(train_loader)\n",
        "\n",
        "def val_epoch():\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, lengths, tin, tout in val_loader:\n",
        "            src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "            tin, tout = tin.to(DEVICE), tout.to(DEVICE)\n",
        "            pred = model(src, lengths, tin)\n",
        "            loss = criterion(pred.reshape(-1, pred.size(-1)), tout.reshape(-1))\n",
        "            total += loss.item()\n",
        "    return total / len(val_loader)"
      ],
      "metadata": {
        "id": "nME6q5gDg-P0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "EPOCHS = 15  # o las que estés usando\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr = train_epoch()\n",
        "    vl = val_epoch()\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(vl)\n",
        "    print(f\"Epoch {ep} | Train: {tr:.4f} | Val: {vl:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHp1Ncm9g_qE",
        "outputId": "571a986d-b7b2-4b62-cb43-4b452a707510"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train 5.0673 | Val 4.3943\n",
            "Epoch 2 | Train 4.0636 | Val 3.9005\n",
            "Epoch 3 | Train 3.6149 | Val 3.6591\n",
            "Epoch 4 | Train 3.2950 | Val 3.5125\n",
            "Epoch 5 | Train 3.0295 | Val 3.4302\n",
            "Epoch 6 | Train 2.7867 | Val 3.3902\n",
            "Epoch 7 | Train 2.5598 | Val 3.3754\n",
            "Epoch 8 | Train 2.3420 | Val 3.3834\n",
            "Epoch 9 | Train 2.1375 | Val 3.4142\n",
            "Epoch 10 | Train 1.9422 | Val 3.4659\n",
            "Epoch 11 | Train 1.7594 | Val 3.5332\n",
            "Epoch 12 | Train 1.5878 | Val 3.5930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Evolución de la pérdida (Train vs Val)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KUUNDmMlXH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    \"hola, ¿cómo estás?\",\n",
        "    \"mañana voy a estudiar en la biblioteca\",\n",
        "    \"me gusta la comida francesa\",\n",
        "    \"estoy aprendiendo modelos de traducción automática\"\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    print(\"ES:\", t)\n",
        "    print(\"FR:\", model.translate(t))\n",
        "    print(\"-\"*40)"
      ],
      "metadata": {
        "id": "1MRpkxijhBEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyps, refs = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, lengths, tin, tout in test_loader:\n",
        "        src, lengths = src.to(DEVICE), lengths.to(DEVICE)\n",
        "        for i in range(src.size(0)):\n",
        "            ids = src[i][:lengths[i]].tolist()\n",
        "            if EOS in ids:\n",
        "                ids = ids[1:ids.index(EOS)]\n",
        "            else:\n",
        "                ids = ids[1:]\n",
        "\n",
        "            src_txt = sp_src.decode(ids)\n",
        "            hyp = model.translate(src_txt)\n",
        "            gold = sp_tgt.decode([x for x in tout[i].tolist() if x not in [PAD,BOS,EOS]])\n",
        "\n",
        "            hyps.append(hyp)\n",
        "            refs.append([gold])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "print(\"BLEU:\", bleu.score)"
      ],
      "metadata": {
        "id": "MxjMQI-9hCeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **========= MODELO TRANSFORMER (MINI TRANSFORMER) =========**"
      ],
      "metadata": {
        "id": "UhT_BsJjhbxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (seq_len, batch, d_model)\n",
        "        seq_len = x.size(0)\n",
        "        return x + self.pe[:seq_len]"
      ],
      "metadata": {
        "id": "kVZD_i3mhrOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerNMT(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        d_model=256,\n",
        "        nhead=4,\n",
        "        num_encoder_layers=2,\n",
        "        num_decoder_layers=2,\n",
        "        dim_feedforward=512,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, d_model, padding_idx=PAD)\n",
        "        self.tgt_embed = nn.Embedding(tgt_vocab_size, d_model, padding_idx=PAD)\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(d_model)\n",
        "        self.pos_decoder = PositionalEncoding(d_model)\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=False  # trabajamos como (S, B, E)\n",
        "        )\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "\n",
        "    def make_src_key_padding_mask(self, src):\n",
        "        # src: (B, S)\n",
        "        return (src == PAD)  # (B, S) True donde hay PAD\n",
        "\n",
        "    def make_tgt_key_padding_mask(self, tgt):\n",
        "        # tgt: (B, T)\n",
        "        return (tgt == PAD)\n",
        "\n",
        "    def make_tgt_subsequent_mask(self, size):\n",
        "        # Máscara triangular inferior para impedir ver el futuro\n",
        "        mask = torch.triu(torch.ones(size, size) == 1, diagonal=1)\n",
        "        # True donde se debe enmascarar\n",
        "        return mask  # (T, T) bool\n",
        "\n",
        "    def forward(self, src, tgt_in):\n",
        "        # src: (B, S), tgt_in: (B, T)\n",
        "        src_key_padding_mask = self.make_src_key_padding_mask(src)  # (B, S)\n",
        "        tgt_key_padding_mask = self.make_tgt_key_padding_mask(tgt_in)  # (B, T)\n",
        "        tgt_mask = self.make_tgt_subsequent_mask(tgt_in.size(1)).to(src.device)  # (T, T)\n",
        "\n",
        "        # Embedding + pos encoding\n",
        "        src_emb = self.src_embed(src) * math.sqrt(self.d_model)  # (B, S, E)\n",
        "        tgt_emb = self.tgt_embed(tgt_in) * math.sqrt(self.d_model)  # (B, T, E)\n",
        "\n",
        "        # Pasar a (S, B, E)\n",
        "        src_emb = src_emb.transpose(0, 1)  # (S, B, E)\n",
        "        tgt_emb = tgt_emb.transpose(0, 1)  # (T, B, E)\n",
        "\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        tgt_emb = self.pos_decoder(tgt_emb)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src=src_emb,\n",
        "            tgt=tgt_emb,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=src_key_padding_mask,\n",
        "        )  # (T, B, E)\n",
        "\n",
        "        output = output.transpose(0, 1)  # (B, T, E)\n",
        "        logits = self.fc_out(output)     # (B, T, vocab_tgt)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def translate(self, text, max_len=40):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            src_ids = torch.tensor([enc_src(text)], device=DEVICE)  # (1, S)\n",
        "            src_key_padding_mask = self.make_src_key_padding_mask(src_ids)\n",
        "            src_emb = self.src_embed(src_ids) * math.sqrt(self.d_model)\n",
        "            src_emb = src_emb.transpose(0, 1)  # (S, 1, E)\n",
        "            src_emb = self.pos_encoder(src_emb)\n",
        "\n",
        "            memory = self.transformer.encoder(\n",
        "                src_emb,\n",
        "                src_key_padding_mask=src_key_padding_mask\n",
        "            )  # (S, 1, E)\n",
        "\n",
        "            # Decoding autoregresivo\n",
        "            generated = [BOS]\n",
        "            for _ in range(max_len):\n",
        "                tgt_in = torch.tensor([generated], device=DEVICE)  # (1, len)\n",
        "                tgt_emb = self.tgt_embed(tgt_in) * math.sqrt(self.d_model)\n",
        "                tgt_emb = tgt_emb.transpose(0, 1)  # (T, 1, E)\n",
        "                tgt_emb = self.pos_decoder(tgt_emb)\n",
        "\n",
        "                tgt_mask = self.make_tgt_subsequent_mask(tgt_in.size(1)).to(DEVICE)\n",
        "                tgt_key_padding_mask = self.make_tgt_key_padding_mask(tgt_in)\n",
        "\n",
        "                out = self.transformer.decoder(\n",
        "                    tgt_emb,\n",
        "                    memory,\n",
        "                    tgt_mask=tgt_mask,\n",
        "                    tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                    memory_key_padding_mask=src_key_padding_mask,\n",
        "                )  # (T, 1, E)\n",
        "\n",
        "                logits = self.fc_out(out[-1])  # (1, vocab)\n",
        "                next_token = logits.argmax(-1).item()\n",
        "\n",
        "                if next_token == EOS:\n",
        "                    break\n",
        "                generated.append(next_token)\n",
        "\n",
        "            return dec_tgt(generated)"
      ],
      "metadata": {
        "id": "08szt4awhtKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 256\n",
        "nhead = 4\n",
        "num_enc_layers = 2\n",
        "num_dec_layers = 2\n",
        "ff_dim = 512\n",
        "dropout = 0.1\n",
        "\n",
        "model = TransformerNMT(\n",
        "    src_vocab_size=sp_src.get_piece_size(),\n",
        "    tgt_vocab_size=sp_tgt.get_piece_size(),\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_enc_layers,\n",
        "    num_decoder_layers=num_dec_layers,\n",
        "    dim_feedforward=ff_dim,\n",
        "    dropout=dropout\n",
        ").to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "print(\"Modelo listo. Parámetros entrenables:\",\n",
        "      sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ],
      "metadata": {
        "id": "JT__ezsShunI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch():\n",
        "    model.train()\n",
        "    total = 0\n",
        "    for src, tgt_in, tgt_out in train_loader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt_in = tgt_in.to(DEVICE)\n",
        "        tgt_out = tgt_out.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(src, tgt_in)  # (B, T, vocab)\n",
        "\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            tgt_out.reshape(-1)\n",
        "        )\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item()\n",
        "    return total / len(train_loader)\n",
        "\n",
        "def eval_epoch():\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for src, tgt_in, tgt_out in val_loader:\n",
        "            src = src.to(DEVICE)\n",
        "            tgt_in = tgt_in.to(DEVICE)\n",
        "            tgt_out = tgt_out.to(DEVICE)\n",
        "\n",
        "            logits = model(src, tgt_in)\n",
        "            loss = criterion(\n",
        "                logits.reshape(-1, logits.size(-1)),\n",
        "                tgt_out.reshape(-1)\n",
        "            )\n",
        "            total += loss.item()\n",
        "    return total / len(val_loader)"
      ],
      "metadata": {
        "id": "O5SpUPQmhwOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "EPOCHS = 15  # o las que estés usando\n",
        "\n",
        "for ep in range(1, EPOCHS + 1):\n",
        "    tr = train_epoch()\n",
        "    vl = val_epoch()\n",
        "    train_losses.append(tr)\n",
        "    val_losses.append(vl)\n",
        "    print(f\"Epoch {ep} | Train: {tr:.4f} | Val: {vl:.4f}\")"
      ],
      "metadata": {
        "id": "NoWwoNVbhxuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Val Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Evolución de la pérdida (Train vs Val)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7kqBteLvldN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = [\n",
        "    \"hola, ¿cómo estás?\",\n",
        "    \"nos vemos mañana por la mañana\",\n",
        "    \"me gusta la comida francesa\",\n",
        "    \"estoy aprendiendo modelos de traducción automática\",\n",
        "    \"el libro está sobre la mesa\",\n",
        "]\n",
        "\n",
        "for t in tests:\n",
        "    print(\"ES:\", t)\n",
        "    print(\"FR:\", model.translate(t))\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "_0P23ab5h0Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "hyps, refs = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for src, tgt_in, tgt_out in test_loader:\n",
        "        src = src.to(DEVICE)\n",
        "        for i in range(src.size(0)):\n",
        "            src_ids = src[i].tolist()\n",
        "            # cortar en EOS y quitar BOS\n",
        "            if EOS in src_ids:\n",
        "                src_ids = src_ids[1:src_ids.index(EOS)]\n",
        "            else:\n",
        "                src_ids = src_ids[1:]\n",
        "            src_text = sp_src.decode(src_ids)\n",
        "\n",
        "            hyp = model.translate(src_text)\n",
        "\n",
        "            # referencia (tgt_out está ya desplazado, quitamos PAD/BOS/EOS)\n",
        "            tgt_ids = tgt_out[i].tolist()\n",
        "            ref_ids = [x for x in tgt_ids if x not in [PAD, BOS, EOS]]\n",
        "            ref_text = sp_tgt.decode(ref_ids)\n",
        "\n",
        "            hyps.append(hyp)\n",
        "            refs.append([ref_text])\n",
        "\n",
        "bleu = sacrebleu.corpus_bleu(hyps, list(zip(*refs)))\n",
        "print(\"BLEU:\", bleu.score)"
      ],
      "metadata": {
        "id": "QXcQJiAwh1zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COMPARACIÓN GRÁFICA DEL SCORE BLEU DE TODOS LOS MODELOS**"
      ],
      "metadata": {
        "id": "-zt1iKcJl0m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smooth = SmoothingFunction().method1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRcGFCl4l_CK",
        "outputId": "e82f7e65-2d9a-44fc-9011-06b2fa690da9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_test = df.sample(200, random_state=42)  # 200 frases de prueba\n",
        "src_test = df_test[\"src\"].tolist()\n",
        "tgt_test = df_test[\"tgt\"].tolist()  # referencias reales\n"
      ],
      "metadata": {
        "id": "ch0PeiHrmBPa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu_model(translate_fn, src_list, tgt_list):\n",
        "    scores = []\n",
        "    for s, t in zip(src_list, tgt_list):\n",
        "        pred = translate_fn(s)  # Traducción producida por ese modelo\n",
        "        reference = t.split()\n",
        "        hypothesis = pred.split()\n",
        "\n",
        "        score = sentence_bleu(\n",
        "            [reference],\n",
        "            hypothesis,\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    return sum(scores) / len(scores)\n"
      ],
      "metadata": {
        "id": "DuzA5_6wmEDa"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_rnn        = compute_bleu_model(translate_rnn, src_test, tgt_test)\n",
        "bleu_lstm       = compute_bleu_model(translate_lstm, src_test, tgt_test)\n",
        "bleu_gru        = compute_bleu_model(translate_gru, src_test, tgt_test)\n",
        "bleu_transform  = compute_bleu_model(translate_transformer, src_test, tgt_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "2egQhwt_mFiz",
        "outputId": "329c8ddd-1261-4bd1-d5f3-8a4f398f1c83"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'translate_rnn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-429944661.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbleu_rnn\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbleu_lstm\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_lstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbleu_gru\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_gru\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbleu_transform\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mcompute_bleu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_transformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'translate_rnn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"===== RESULTADOS BLEU =====\")\n",
        "print(f\"RNN Simple:           {bleu_rnn:.4f}\")\n",
        "print(f\"LSTM + Atención:      {bleu_lstm:.4f}\")\n",
        "print(f\"GRU + Atención:       {bleu_gru:.4f}\")\n",
        "print(f\"Transformer:          {bleu_transform:.4f}\")\n"
      ],
      "metadata": {
        "id": "Sa6nsP6-mHlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "modelos = [\"RNN\", \"LSTM + Att\", \"GRU + Att\", \"Transformer\"]\n",
        "bleus = [bleu_rnn, bleu_lstm, bleu_gru, bleu_transform]\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(modelos, bleus, color=[\"#4a6cff\",\"#00b48a\",\"#ffaf40\",\"#ff637d\"])\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.title(\"Comparación del BLEU por Modelo\")\n",
        "for i, v in enumerate(bleus):\n",
        "    plt.text(i, v + 0.002, f\"{v:.3f}\", ha=\"center\", fontsize=12)\n",
        "plt.ylim(0, max(bleus) + 0.01)\n",
        "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9Zt7WSPAmJID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}